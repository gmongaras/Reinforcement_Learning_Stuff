{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10          # Number of arms (distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the true reward for each action\n",
    "action_values = np.random.randn(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the values for each k. It should be discrete points, not a line\n",
    "plt.plot(action_values, 'o')\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward is sampled from a distribution with mean\n",
    "# q*(a), the mean of the given action\n",
    "def reward_model(action_values, action):\n",
    "    return np.random.randn() + action_values[action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal action is the one with the greatest mean\n",
    "optimal_action = action_values.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Epsilon Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_search(action_values, epsilon, num_iterations=1000, init=0):\n",
    "    num_actions = action_values.shape[0]\n",
    "    \n",
    "    # Q value / estimated reward for each action is 0\n",
    "    q_values = np.zeros(num_actions) + init\n",
    "    # Number of times each action has been taken is 0\n",
    "    n_values = np.zeros(num_actions)\n",
    "    \n",
    "    # Average reward\n",
    "    avg_reward = 0\n",
    "    \n",
    "    for it in range(1, num_iterations+1):\n",
    "        # Random action with probability epsilon\n",
    "        sample = np.random.rand()\n",
    "        if sample < epsilon:\n",
    "            action = np.random.randint(0, num_actions)\n",
    "        else:\n",
    "            action = np.argmax(q_values)\n",
    "            \n",
    "        # Get the reward for the given action\n",
    "        reward = reward_model(action_values, action)\n",
    "        \n",
    "        avg_reward += reward\n",
    "        \n",
    "        # Update the q values and n values\n",
    "        n_values[action] += 1\n",
    "        q_values[action] = q_values[action] + 1/n_values[action] * (reward - q_values[action])\n",
    "        \n",
    "    return q_values, avg_reward/num_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.010974502899930573\n",
      "[-0.17171225 -0.39328213 -1.8378916   0.42082669  0.97542771 -2.0896267\n",
      " -0.37673618 -1.34289793 -0.00615886 -0.33282939]\n",
      "[-0.15692823 -0.39549824 -1.66959115  0.38393378  0.97873751 -1.98213694\n",
      " -0.25675163 -1.12404535 -0.0785878  -0.3598798 ]\n",
      "0.8254215707776955\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "pred, avg_reward = epsilon_search(action_values, 0.1, 10000)\n",
    "# MSE\n",
    "print(\n",
    "    \"MSE\",\n",
    "    ((pred-action_values)**2).mean()\n",
    ")\n",
    "print(pred)\n",
    "print(action_values)\n",
    "print(avg_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upper Confidence Bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UCB_search(action_values, num_iterations=1000, c=1):\n",
    "    num_actions = action_values.shape[0]\n",
    "    \n",
    "    # Q value / estimated reward for each action is 0\n",
    "    q_values = np.zeros(num_actions)\n",
    "    # Number of times each action has been taken is 0\n",
    "    n_values = np.zeros(num_actions)\n",
    "    \n",
    "    # Average reward\n",
    "    avg_reward = 0\n",
    "    \n",
    "    for it in range(1, num_iterations+1):\n",
    "        # Select \"best\" action according to UCB\n",
    "        actions = np.zeros(num_actions)\n",
    "        mask = n_values > 0\n",
    "        actions[mask] = q_values[mask] + c*np.sqrt(np.log(it)/n_values[mask])\n",
    "        actions[~mask] = q_values[~mask]\n",
    "        action = np.argmax(actions)\n",
    "            \n",
    "        # Get the reward for the given action\n",
    "        reward = reward_model(action_values, action)\n",
    "        \n",
    "        avg_reward += reward\n",
    "        \n",
    "        # Update the q values and n values\n",
    "        n_values[action] += 1\n",
    "        q_values[action] = q_values[action] + 1/n_values[action] * (reward - q_values[action])\n",
    "        \n",
    "    return q_values, avg_reward/num_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.8345989561118268\n",
      "[-0.63867846 -0.80974847 -2.62854083 -0.89222461  0.99225047  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "[-0.15692823 -0.39549824 -1.66959115  0.38393378  0.97873751 -1.98213694\n",
      " -0.25675163 -1.12404535 -0.0785878  -0.3598798 ]\n",
      "0.9784194868745173\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "pred, avg_reward = UCB_search(action_values, 1000, 0.5)\n",
    "# MSE\n",
    "print(\n",
    "    \"MSE\",\n",
    "    ((pred-action_values)**2).mean()\n",
    ")\n",
    "print(pred)\n",
    "print(action_values)\n",
    "print(avg_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Ascent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grad_Ascent(action_values, num_iterations=1000, init=0, step_size=0.1):\n",
    "    num_actions = action_values.shape[0]\n",
    "    \n",
    "    # Preference for each action\n",
    "    preferences = np.zeros(num_actions) + init\n",
    "    \n",
    "    # Average reward\n",
    "    avg_reward = 0\n",
    "    \n",
    "    # Number of times the optimal action was chosen\n",
    "    \n",
    "    for it in range(1, num_iterations+1):\n",
    "        # Calculate the probability of each action\n",
    "        probs = softmax(preferences)\n",
    "        \n",
    "        # Sample ditribution\n",
    "        action = np.random.choice(np.arange(0, num_actions), p=probs)\n",
    "        \n",
    "        # Get the reward for the given action\n",
    "        reward = reward_model(action_values, action)\n",
    "        \n",
    "        # Update model\n",
    "        for a in range(0, num_actions):\n",
    "            if a == action:\n",
    "                # Step in the direciton of this action\n",
    "                preferences[a] = preferences[a] + step_size * (reward - avg_reward) * (1-probs[a])\n",
    "            else:\n",
    "                # Step away from the other actions\n",
    "                preferences[a] = preferences[a] - step_size * (reward - avg_reward) * probs[a]\n",
    "                \n",
    "        # Update average\n",
    "        avg_reward += reward/num_iterations\n",
    "        \n",
    "    return preferences, avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9788688662056394\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "pred, avg_reward = Grad_Ascent(action_values, 100000)\n",
    "# MSE\n",
    "print(avg_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:08<00:00, 234.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num steps: 100\n",
      "Greedy  | Average reward: 0.5290 | Percent optimal action: 0.4645\n",
      "Epsilon | Average reward: 0.5099 | Percent optimal action: 0.7045\n",
      "UCB     | Average reward: 0.3095 | Percent optimal action: 0.2015\n",
      "Grad    | Average reward: -0.0596 | Percent optimal action: 0.8890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:53<00:00, 37.25it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num steps: 200\n",
      "Greedy  | Average reward: 0.5905 | Percent optimal action: 0.4805\n",
      "Epsilon | Average reward: 0.5991 | Percent optimal action: 0.8095\n",
      "UCB     | Average reward: 0.3873 | Percent optimal action: 0.2015\n",
      "Grad    | Average reward: 0.3078 | Percent optimal action: 0.9710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:25<00:00, 77.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num steps: 300\n",
      "Greedy  | Average reward: 0.6163 | Percent optimal action: 0.4965\n",
      "Epsilon | Average reward: 0.6478 | Percent optimal action: 0.8660\n",
      "UCB     | Average reward: 0.4274 | Percent optimal action: 0.2145\n",
      "Grad    | Average reward: 0.4949 | Percent optimal action: 0.9830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:32<00:00, 61.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num steps: 400\n",
      "Greedy  | Average reward: 0.6150 | Percent optimal action: 0.4790\n",
      "Epsilon | Average reward: 0.6827 | Percent optimal action: 0.9195\n",
      "UCB     | Average reward: 0.4465 | Percent optimal action: 0.2120\n",
      "Grad    | Average reward: 0.6019 | Percent optimal action: 0.9905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:57<00:00, 34.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num steps: 500\n",
      "Greedy  | Average reward: 0.6257 | Percent optimal action: 0.4870\n",
      "Epsilon | Average reward: 0.7035 | Percent optimal action: 0.9380\n",
      "UCB     | Average reward: 0.4633 | Percent optimal action: 0.2160\n",
      "Grad    | Average reward: 0.6749 | Percent optimal action: 0.9915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:20<00:00, 24.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num steps: 600\n",
      "Greedy  | Average reward: 0.6270 | Percent optimal action: 0.4760\n",
      "Epsilon | Average reward: 0.7173 | Percent optimal action: 0.9520\n",
      "UCB     | Average reward: 0.4574 | Percent optimal action: 0.2000\n",
      "Grad    | Average reward: 0.7199 | Percent optimal action: 0.9950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:49<00:00, 18.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num steps: 700\n",
      "Greedy  | Average reward: 0.6449 | Percent optimal action: 0.4965\n",
      "Epsilon | Average reward: 0.7319 | Percent optimal action: 0.9595\n",
      "UCB     | Average reward: 0.4697 | Percent optimal action: 0.2085\n",
      "Grad    | Average reward: 0.7553 | Percent optimal action: 0.9970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:05<00:00, 30.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num steps: 800\n",
      "Greedy  | Average reward: 0.6338 | Percent optimal action: 0.4845\n",
      "Epsilon | Average reward: 0.7393 | Percent optimal action: 0.9695\n",
      "UCB     | Average reward: 0.4698 | Percent optimal action: 0.2030\n",
      "Grad    | Average reward: 0.7808 | Percent optimal action: 0.9970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:13<00:00, 27.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num steps: 900\n",
      "Greedy  | Average reward: 0.6438 | Percent optimal action: 0.4950\n",
      "Epsilon | Average reward: 0.7533 | Percent optimal action: 0.9815\n",
      "UCB     | Average reward: 0.4733 | Percent optimal action: 0.1955\n",
      "Grad    | Average reward: 0.8006 | Percent optimal action: 0.9940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:21<00:00, 24.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num steps: 1000\n",
      "Greedy  | Average reward: 0.6371 | Percent optimal action: 0.4820\n",
      "Epsilon | Average reward: 0.7589 | Percent optimal action: 0.9860\n",
      "UCB     | Average reward: 0.4782 | Percent optimal action: 0.2015\n",
      "Grad    | Average reward: 0.8153 | Percent optimal action: 0.9975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_eval_steps = 2000\n",
    "num_step_ranges = np.arange(100, 1001, 100)\n",
    "\n",
    "\n",
    "# Evaluate each algorithm to see how well it does compared to the others\n",
    "for num_steps in num_step_ranges:\n",
    "    # Algo info\n",
    "    greedy_reward, greedy_times_selected = [], []\n",
    "    epsilon_reward, epsilon_times_selected = [], []\n",
    "    UCB_reward, UCB_times_selected = [], []\n",
    "    grad_reward, grad_times_selected = [], []\n",
    "    \n",
    "    # Collect data for num_eval_steps times\n",
    "    for step in tqdm(range(num_eval_steps)):\n",
    "        # Greedy search\n",
    "        pred, avg_reward = epsilon_search(action_values, epsilon=0.0, num_iterations=num_steps)\n",
    "        greedy_reward.append(avg_reward)\n",
    "        greedy_times_selected.append(pred.argmax() == optimal_action)\n",
    "        \n",
    "        # Epsilon search\n",
    "        pred, avg_reward = epsilon_search(action_values, epsilon=0.1, num_iterations=num_steps)\n",
    "        epsilon_reward.append(avg_reward)\n",
    "        epsilon_times_selected.append(pred.argmax() == optimal_action)\n",
    "        \n",
    "        # UCB search\n",
    "        pred, avg_reward = UCB_search(action_values, num_iterations=num_steps, c=0.5)\n",
    "        UCB_reward.append(avg_reward)\n",
    "        UCB_times_selected.append(pred.argmax() == optimal_action)\n",
    "        \n",
    "        # Grad search\n",
    "        pred, avg_reward = Grad_Ascent(action_values, num_iterations=num_steps)\n",
    "        grad_reward.append(avg_reward)\n",
    "        grad_times_selected.append(pred.argmax() == optimal_action)\n",
    "    \n",
    "    print(\"Num steps:\", num_steps)\n",
    "    print(f\"Greedy  | Average reward: {np.mean(greedy_reward):0.4f} | Percent optimal action: {np.mean(greedy_times_selected):0.4f}\")\n",
    "    print(f\"Epsilon | Average reward: {np.mean(epsilon_reward):0.4f} | Percent optimal action: {np.mean(epsilon_times_selected):0.4f}\")\n",
    "    print(f\"UCB     | Average reward: {np.mean(UCB_reward):0.4f} | Percent optimal action: {np.mean(UCB_times_selected):0.4f}\")\n",
    "    print(f\"Grad    | Average reward: {np.mean(grad_reward):0.4f} | Percent optimal action: {np.mean(grad_times_selected):0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
